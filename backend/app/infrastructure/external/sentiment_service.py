import requests
import re
import unicodedata
import logging
from app.presentation.config import HF_TOKEN, HF_SENTIMENT_MODEL_URL, HF_TOXICITY_MODEL_URL
from app.application.dto.sentiment_analysis_result_dto import SentimentAnalysisResultDTO
from app.application.dto.review_dto import ReviewDTO
from app.infrastructure.external.text_profanity_service import TextProfanityService
import time
class SentimentService:
    MAX_RETRIES = 3
    INITIAL_WAIT = 2.0  # Ø«ÙˆØ§Ù†ÙŠ    @staticmethod
    def clean_text(text: str) -> str:
        try:
            if not text or not isinstance(text, str):
                return ""
            text = unicodedata.normalize('NFKC', text)
            text = re.sub(r'[\u064B-\u065F\u0670]', '', text)
            text = re.sub(r'[\u0640]', '', text)
            text = re.sub(r'[Ø£Ø¥Ø¢]', 'Ø§', text)
            text = re.sub(r'(.)\1{2,}', r'\1\1', text)
            valid_chars_pattern = r'[^a-zA-Z0-9\u0600-\u06FF\s.,!?Ø›ØŸ\:\_\-\(\)\U00010000-\U0010ffff\u2600-\u27BF]'
            text = re.sub(valid_chars_pattern, '', text).strip()
            text = re.sub(r'\s+', ' ', text).strip()

            return text

        except Exception as e:
            logging.error(f"Error cleaning text: {e}")
            return str(text) if text else ""

        except Exception as e:
            logging.error(f"Error cleaning text: {e}")
            return str(text) if text else ""
    # @staticmethod
    # def analyze_sentiment(text: str) -> str:
    #     if not text or not text.strip():
    #         return "Ù…Ø­Ø§ÙŠØ¯"

    #     headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    #     url = HF_SENTIMENT_MODEL_URL
    #     try:
    #         response = requests.post(url, headers=headers, json={"inputs": text})
    #         if response.status_code == 200:
    #             result = response.json()
    #             label = None

    #             if isinstance(result, list) and result:
    #                 first_element = result[0]
    #                 if isinstance(first_element, list) and first_element:
    #                     if isinstance(first_element[0], dict):
    #                         label = first_element[0].get("label", "neutral")
    #                 elif isinstance(first_element, dict): 
    #                     label = first_element.get("label", "neutral")        
    #                 else:          
    #                         # ØªØ³Ø¬ÙŠÙ„ Ø®Ø·Ø£/ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹
    #                     logging.warning(f"Sentiment API returned list but first element is not a dict: {first_element}")
    #             if label:
    #                 mapping = {
    #                     "positive": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
    #                     "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
    #                     "label_1": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
    #                     "negative": "Ø³Ù„Ø¨ÙŠ",
    #                     "Ø³Ù„Ø¨ÙŠ": "Ø³Ù„Ø¨ÙŠ",
    #                     "label_0": "Ø³Ù„Ø¨ÙŠ",
    #                     "neutral": "Ù…Ø­Ø§ÙŠØ¯"
    #                 }
    #                 return mapping.get(label.lower(), "Ù…Ø­Ø§ÙŠØ¯")

    #         else:
    #             logging.error(f"HuggingFace Sentiment API error: {response.status_code} - {response.text}")
    #     except Exception as e:
    #         logging.error(f"Sentiment analysis error: {e}")
    #     return "Ù…Ø­Ø§ÙŠØ¯"
    @staticmethod
    def analyze_sentiment(text: str) -> str:
        if not text or not text.strip():
            return "Ù…Ø­Ø§ÙŠØ¯"

        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        url = HF_SENTIMENT_MODEL_URL
        payload = {"inputs": text}

        for attempt in range(SentimentService.MAX_RETRIES):
            try:
                response = requests.post(url, headers=headers, json=payload, timeout=10)
                if response.status_code == 200:
                    return SentimentService._parse_response_to_string(response.json())
                elif response.status_code == 503:
                    error_data = response.json()
                    estimated_time = error_data.get("estimated_time", SentimentService.INITIAL_WAIT)
                    logging.info(f"Model loading... Waiting {estimated_time:.2f}s (Attempt {attempt+1})")
                    time.sleep(estimated_time)
                    continue 
                else:
                    logging.error(f"HF API Error {response.status_code}: {response.text}")
                    break

            except requests.exceptions.Timeout:
                logging.warning(f"HF API Timeout (Attempt {attempt+1})")
            except Exception as e:
                logging.error(f"Connection Error: {e}")
                break
        return "Ù…Ø­Ø§ÙŠØ¯"
    @staticmethod
    def _parse_response_to_string(result) -> str:
        try:
            predictions = []
            if isinstance(result, list) and result:
                if isinstance(result[0], list):
                    predictions = result[0]
                elif isinstance(result[0], dict):
                    predictions = result
            
            if not predictions:
                return "Ù…Ø­Ø§ÙŠØ¯"

            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ø£Ø®Ø° Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø«Ù‚Ø©
            top_prediction = sorted(predictions, key=lambda x: x.get('score', 0), reverse=True)[0]
            raw_label = top_prediction.get('label', 'neutral').lower()

            mapping = {
                "positive": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                "pos": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                "label_2": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                "label_1": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                
                "negative": "Ø³Ù„Ø¨ÙŠ",
                "neg": "Ø³Ù„Ø¨ÙŠ",
                "label_0": "Ø³Ù„Ø¨ÙŠ",
                
                "neutral": "Ù…Ø­Ø§ÙŠØ¯",
                "neu": "Ù…Ø­Ø§ÙŠØ¯",
                "label_1": "Ù…Ø­Ø§ÙŠØ¯" 
            }
            return mapping.get(raw_label, "Ù…Ø­Ø§ÙŠØ¯")

        except Exception as e:
            logging.error(f"Parsing Error: {e}")
            return "Ù…Ø­Ø§ÙŠØ¯"

    @staticmethod
    def analyze_toxicity(text: str) -> str:
        if not text or not text.strip():
            return "non-toxic"

        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        url = HF_TOXICITY_MODEL_URL

        toxic_label = "Ø´ØªØ§Ø¦Ù… ÙˆÙƒÙ„Ø§Ù… Ø¨Ø°ÙŠØ¡ ÙˆÙ…Ù‡ÙŠÙ†"
        safe_label = "Ù†Ù‚Ø¯ Ù…Ø­ØªØ±Ù… ÙˆÙƒÙ„Ø§Ù… Ø¹Ø§Ø¯ÙŠ"

        payload = {
            "inputs": text,
            "parameters": {
                "candidate_labels": [toxic_label, safe_label],
                "multi_label": False  
            }
        }

        for attempt in range(SentimentService.MAX_RETRIES):
            try:
                response = requests.post(url, headers=headers, json=payload, timeout=70)
                if response.status_code == 200:
                    return SentimentService._parse_toxicity_response(
                        response.json(), 
                        toxic_label
                    )

                elif response.status_code == 503:
                    error_data = response.json()
                    estimated_time = error_data.get("estimated_time", SentimentService.INITIAL_WAIT)
                    logging.info(f"ğŸ›¡ï¸ Toxicity Model loading... Waiting {estimated_time:.2f}s")
                    time.sleep(estimated_time)
                    continue
                else:
                    logging.error(f"âŒ Toxicity API Error {response.status_code}: {response.text}")
                    break

            except Exception as e:
                logging.error(f"âŒ Toxicity Check Error: {e}")
                break
        return "uncertain"
    @staticmethod
    def _parse_toxicity_response(result, target_toxic_label) -> str:
        try:
            if isinstance(result, list):
                result = result[0] if result else {}

            if not isinstance(result, dict):
                return "uncertain"

            labels = result.get("labels", [])
            scores = result.get("scores", [])

            if not labels or not scores:
                return "uncertain"

            top_label = labels[0]
            top_score = scores[0]

            if top_label != target_toxic_label:
                if top_score < 0.60: 
                    return "uncertain"
                return "non-toxic"
            if top_label == target_toxic_label:
                if top_score >= 0.70:
                    return "toxic"
                elif top_score >= 0.50:
                    return "uncertain"
                else:
                    return "non-toxic"

            return "non-toxic"

        except Exception as e:
            logging.error(f"Parsing Toxicity Error: {e}")
            return "uncertain"

    @staticmethod
    def detect_review_quality(enjoy_most: str, improve_product: str, additional_feedback: str, rating: int = 0, pre_calculated_toxicity: str = None) -> dict:
        flags = []
        quality_score = 1.0

        parts = [p.strip() for p in [enjoy_most, improve_product, additional_feedback] if p and p.strip()]
        all_text = " ".join(parts)

        # Ø­Ø§Ù„Ø© Ø®Ø§ØµØ©: ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ù„Ù†Ø¬ÙˆÙ… ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ù†Øµ)
        if not all_text or len(all_text) < 3:
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ù„Ù†Ø¬ÙˆÙ…ØŒ Ù†Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            if rating > 0:
                flags_for_stars = ['stars_only']
                # Ø¥Ø¶Ø§ÙØ© flag Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ù„Ù†Ø¬ÙˆÙ… (Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
                if rating <= 2:
                    flags_for_stars.append('negative_stars')
                elif rating >= 4:
                    flags_for_stars.append('positive_stars')
                else:
                    flags_for_stars.append('neutral_stars')
                
                return {
                    'quality_score': 1.0,
                    'flags': flags_for_stars,
                    'is_suspicious': False,
                    'toxicity_status': pre_calculated_toxicity or "non-toxic"
                }
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù†Ø¬ÙˆÙ… ÙˆÙ„Ø§ Ù†ØµØŒ Ù†Ø±ÙØ¶
            else:
                return {
                    'quality_score': 0.0,
                    'flags': ['empty_content'],
                    'is_suspicious': True,
                    'toxicity_status': pre_calculated_toxicity or "non-toxic"
                }

        try:
            arabic_chars = sum(1 for c in all_text if '\u0600' <= c <= '\u06FF')
            english_chars = sum(1 for c in all_text if c.isascii() and c.isalpha())
            total_alpha = arabic_chars + english_chars

            if total_alpha < len(all_text) * 0.3:
                flags.append('gibberish_content')
                quality_score -= 0.3
        except Exception as e:
            logging.error(f"Language detection error: {e}")

        words = all_text.split()
        if len(words) > 200 or total_alpha > 500:
            flags.append('too_long')
            quality_score -= 0.1

        if re.search(r'(.)\1{4,}', all_text):
            flags.append('repetitive_characters')
            quality_score -= 0.2

        special_chars = sum(1 for c in all_text if not c.isalnum() and not c.isspace() and c not in '.,!?Ø›ØŒ')
        if special_chars > len(all_text) * 0.2:
            flags.append('excessive_special_chars')
            quality_score -= 0.2

        toxicity_score = pre_calculated_toxicity or SentimentService.analyze_toxicity(all_text)
        if toxicity_score == "toxic":
            flags.append('high_toxicity')
            quality_score -= 0.4
        elif toxicity_score == "uncertain":
            flags.append('possible_toxicity')
            quality_score -= 0.1

        if len(words) < 2:
            flags.append('too_short')
            quality_score -= 0.05  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ù…Ù† 0.1 Ø¥Ù„Ù‰ 0.05

        unique_words = set(words)
        # ØªØ·Ø¨ÙŠÙ‚ ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª > 3
        if len(words) > 3 and len(unique_words) < len(words) * 0.4:
            flags.append('repetitive_words')
            # Ø¹Ù‚ÙˆØ¨Ø© Ø£ÙƒØ¨Ø± Ù„Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø´Ø¯ÙŠØ¯
            repetition_ratio = len(unique_words) / len(words)
            if repetition_ratio < 0.25:  # ØªÙƒØ±Ø§Ø± Ø´Ø¯ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ (75%+ Ù…Ù† Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø©)
                quality_score -= 0.4
            else:
                quality_score -= 0.3  # Ø²ÙŠØ§Ø¯Ø© Ù…Ù† 0.2 Ø¥Ù„Ù‰ 0.3

        quality_score = max(0, quality_score)

        # ØªØ­Ø¯ÙŠØ¯ is_suspicious Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± Ø°ÙƒØ§Ø¡Ù‹
        is_suspicious = False

        # Ø­Ø§Ù„Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù„Ù€ suspicious
        if quality_score < 0.4:  # ØªØºÙŠÙŠØ± Ù…Ù† 0.5 Ø¥Ù„Ù‰ 0.4
            is_suspicious = True
        elif toxicity_score == "toxic":  # Ù…Ø­ØªÙˆÙ‰ Ø³Ø§Ù… â†’ suspicious ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
            is_suspicious = True
        elif 'repetitive_words' in flags and quality_score < 0.6:  # ØªÙƒØ±Ø§Ø± + Ø¯Ø±Ø¬Ø© Ù…Ù†Ø®ÙØ¶Ø©
            is_suspicious = True
        elif len(flags) >= 3:  # 3 Ø£Ø¹Ù„Ø§Ù… Ø£Ùˆ Ø£ÙƒØ«Ø±
            is_suspicious = True

        return {
            'quality_score': round(quality_score, 2),
            'flags': flags,
            'is_suspicious': is_suspicious,
            'toxicity_status': toxicity_score
        }

    @staticmethod
    def detect_context_mismatch(text: str, shop_type: str) -> dict:
        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        url = HF_TOXICITY_MODEL_URL

        shop_types_arabic = {
            "Ù…Ø·Ø¹Ù…": "Ù…Ø·Ø¹Ù… ÙˆØ£ÙƒÙ„ ÙˆÙ…Ø´Ø±ÙˆØ¨Ø§Øª",
            "Ù…Ù‚Ù‡Ù‰": "Ù…Ù‚Ù‡Ù‰ ÙˆÙ‚Ù‡ÙˆØ© ÙˆÙ…Ø´Ø±ÙˆØ¨Ø§Øª",
            "Ù…Ø­Ù„ Ù…Ù„Ø§Ø¨Ø³": "Ù…Ù„Ø§Ø¨Ø³ ÙˆØ£Ø²ÙŠØ§Ø¡ ÙˆÙ…ÙˆØ¶Ø©",
            "ØµÙŠØ¯Ù„ÙŠØ©": "ØµÙŠØ¯Ù„ÙŠØ© ÙˆØ£Ø¯ÙˆÙŠØ© ÙˆØ¹Ù„Ø§Ø¬",
            "Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙƒØª": "Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙƒØª ÙˆØªØ³ÙˆÙ‚ ÙˆÙ…Ù†ØªØ¬Ø§Øª",
            "Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª": "Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ£Ø¬Ù‡Ø²Ø© ÙˆØªÙ‚Ù†ÙŠØ©",
            "Ù…ÙƒØªØ¨Ø©": "ÙƒØªØ¨ ÙˆÙ‚Ø±Ø§Ø¡Ø© ÙˆØªØ¹Ù„ÙŠÙ…",
            "Ù…Ø­Ù„ ØªØ¬Ù…ÙŠÙ„": "ØªØ¬Ù…ÙŠÙ„ ÙˆØ´Ø¹Ø± ÙˆØ¨Ø´Ø±Ø©",
            "ØµØ§Ù„Ø© Ø±ÙŠØ§Ø¶ÙŠØ©": "Ø±ÙŠØ§Ø¶Ø© ÙˆØªÙ…Ø§Ø±ÙŠÙ† ÙˆÙ„ÙŠØ§Ù‚Ø©",
            "Ù…Ø¯Ø±Ø³Ø©": "Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ¹Ù„ÙŠÙ… ÙˆØ·Ù„Ø§Ø¨",
            "Ù…Ø³ØªØ´ÙÙ‰": "Ø·Ø¨ ÙˆØ¹Ù„Ø§Ø¬ ÙˆÙ…Ø±Ø¶Ù‰",
            "Ù…Ø­Ø·Ø© ÙˆÙ‚ÙˆØ¯": "ÙˆÙ‚ÙˆØ¯ ÙˆØ³ÙŠØ§Ø±Ø§Øª ÙˆØ¨Ù†Ø²ÙŠÙ†",
            "Ù…ØªØ¬Ø± Ø£Ø¬Ù‡Ø²Ø©": "Ø£Ø¬Ù‡Ø²Ø© ÙˆØ¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØªÙ‚Ù†ÙŠØ©",
            "Ù…Ø­Ù„ Ø£Ù„Ø¹Ø§Ø¨": "Ø£Ù„Ø¹Ø§Ø¨ ÙˆØªØ±ÙÙŠÙ‡ ÙˆØ£Ø·ÙØ§Ù„",
            "Ù…ÙƒØªØ¨ Ø³ÙŠØ§Ø­ÙŠ": "Ø³ÙØ± ÙˆØ³ÙŠØ§Ø­Ø© ÙˆÙÙ†Ø§Ø¯Ù‚",
            "Ù…Ø­Ù„ Ù‡Ø¯Ø§ÙŠØ§": "Ù‡Ø¯Ø§ÙŠØ§ ÙˆØªØ°ÙƒØ§Ø±Ø§Øª ÙˆÙ…Ù†Ø§Ø³Ø¨Ø§Øª",
            "Ù…ØºØ³Ù„Ø© Ù…Ù„Ø§Ø¨Ø³": "ØºØ³ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ ÙˆÙ…Ù„Ø§Ø¨Ø³",
            "Ù…ØªØ¬Ø± Ù‡ÙˆØ§ØªÙ": "Ù‡ÙˆØ§ØªÙ ÙˆÙ…ÙˆØ¨Ø§ÙŠÙ„Ø§Øª ÙˆØªÙ‚Ù†ÙŠØ©",
            "Ø¹Ø§Ù…": "Ù†Ø´Ø§Ø· ØªØ¬Ø§Ø±ÙŠ Ø¹Ø§Ù…"
        }

        target_label = shop_types_arabic.get(shop_type, shop_type)

        # ÙØ¦Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        candidate_labels = [
            target_label,
            "Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙˆØªØ¹Ø§Ù…Ù„ Ø¹Ø§Ù… ÙˆÙ†Ø¸Ø§ÙØ©",
            "Ø±ÙŠØ§Ø¶Ø© ÙˆØ£Ø­Ø¯Ø§Ø« Ø±ÙŠØ§Ø¶ÙŠØ©",
            "Ø³ÙŠØ§Ø³Ø© ÙˆØ£Ø®Ø¨Ø§Ø± Ø¹Ø§Ù…Ø©",
            "ØªØ±ÙÙŠÙ‡ ÙˆÙ…Ø´Ø§Ù‡ÙŠØ±",
            "Ø­ÙŠØ§Ø© Ø´Ø®ØµÙŠØ© Ø£Ùˆ ÙŠÙˆÙ…ÙŠØ§Øª",
            "Ø³ÙŠØ§Ù‚ Ø¢Ø®Ø± ØºÙŠØ± Ù…Ø±ØªØ¨Ø·"
        ]

        payload = {
            "inputs": text,
            "parameters": {
                "candidate_labels": candidate_labels,
                "multi_label": False
            }
        }

        try:
            response = requests.post(url, headers=headers, json=payload)

            if response.status_code == 503:
                logging.info("Model is loading, waiting...")
                import time
                time.sleep(20)
                response = requests.post(url, headers=headers, json=payload)

            if response.status_code == 200:
                result = response.json()
                labels, scores = [], []

                if isinstance(result, dict):
                    labels = result.get("labels", [])
                    scores = result.get("scores", [])
                elif isinstance(result, list):
                    for item in result:
                        if isinstance(item, dict):
                            labels.append(item.get("label"))
                            scores.append(item.get("score"))

                if labels and scores:
                            result_map = {label: score for label, score in zip(labels, scores)}
                            top_label, top_score = labels[0], scores[0]

                            target_score = result_map.get(target_label, 0.0)

                            # Ù…Ù†Ø·Ù‚ mismatch Ø§Ù„Ø¬Ø¯ÙŠØ¯
                            if top_score < 0.4:
                                has_mismatch = True
                                predicted_label = "ØºÙŠØ± Ù…Ø±ØªØ¨Ø·"
                            else:
                                has_mismatch = (top_label != target_label and top_score >= 0.5) or (target_score < 0.5)
                                predicted_label = top_label

                            return {
                                'mismatch_score': round(top_score, 2),
                                'confidence': round(target_score * 100, 2),
                                'reasons': [f"Ø§Ù„Ù†Øµ Ø¨Ø¹ÙŠØ¯ Ø¹Ù† Ø³ÙŠØ§Ù‚ {shop_type}"] if has_mismatch else [],
                                'has_mismatch': has_mismatch,
                                'predicted_label': predicted_label
                            }

            else:
                logging.error(f"HF API Error: {response.status_code} - {response.text}")

        except Exception as e:
            logging.error(f"Context mismatch detection error: {e}")

        return {
            'mismatch_score': 0.0,
            'confidence': 100.0,
            'reasons': 'Ù„Ø§Ø´ÙŠØ¡',
            'has_mismatch': False,
            'predicted_label': "Error"
        }

