import requests
import re
import unicodedata
import logging
from app.config import HF_TOKEN, HF_SENTIMENT_MODEL_URL, HF_TOXICITY_MODEL_URL
from app.dto.sentiment_analysis_result_dto import SentimentAnalysisResultDTO
from app.dto.review_dto import ReviewDTO
from app.services.external.text_profanity_service import TextProfanityService

class SentimentService:

    @staticmethod
    def clean_text(text: str) -> str:
        try:
            if not text or not isinstance(text, str):
                return ""
            text = unicodedata.normalize('NFKC', text)
            text = re.sub(r'[^a-zA-Z0-9\u0660-\u0669\u0600-\u06FF\s.,!?Ø›ØŸ]', '', text).strip()
            return text
        except Exception as e:
            logging.error(f"Error cleaning text: {e}")
            return str(text) if text else ""

    @staticmethod
    def analyze_sentiment(text: str) -> str:
        if not text or not text.strip():
            return "Ù…Ø­Ø§ÙŠØ¯"

        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        url = HF_SENTIMENT_MODEL_URL
        try:
            response = requests.post(url, headers=headers, json={"inputs": text})
            if response.status_code == 200:
                result = response.json()
                label = None

                if isinstance(result, list) and result:
                    first_element = result[0]
                    if isinstance(first_element, list) and first_element:
                        if isinstance(first_element[0], dict):
                            label = first_element[0].get("label", "neutral")
                    elif isinstance(first_element, dict): 
                        label = first_element.get("label", "neutral")        
                    else:          
                            # ØªØ³Ø¬ÙŠÙ„ Ø®Ø·Ø£/ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹
                        logging.warning(f"Sentiment API returned list but first element is not a dict: {first_element}")
                if label:
                    mapping = {
                        "positive": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                        "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                        "label_1": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                        "negative": "Ø³Ù„Ø¨ÙŠ",
                        "Ø³Ù„Ø¨ÙŠ": "Ø³Ù„Ø¨ÙŠ",
                        "label_0": "Ø³Ù„Ø¨ÙŠ",
                        "neutral": "Ù…Ø­Ø§ÙŠØ¯"
                    }
                    return mapping.get(label.lower(), "Ù…Ø­Ø§ÙŠØ¯")

            else:
                logging.error(f"HuggingFace Sentiment API error: {response.status_code} - {response.text}")
        except Exception as e:
            logging.error(f"Sentiment analysis error: {e}")
        return "Ù…Ø­Ø§ÙŠØ¯"

    @staticmethod
    def analyze_toxicity(text: str) -> str:
        if not text or not text.strip():
            return "non-toxic"

        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        url = HF_TOXICITY_MODEL_URL

        toxic_label = "Ø´ØªØ§Ø¦Ù… ÙˆÙƒÙ„Ø§Ù… Ø¨Ø°ÙŠØ¡ ÙˆÙ…Ù‡ÙŠÙ†"
        safe_label = "Ù†Ù‚Ø¯ Ù…Ø­ØªØ±Ù… ÙˆÙƒÙ„Ø§Ù… Ø¹Ø§Ø¯ÙŠ"

        try:
            response = requests.post(
                url,
                headers=headers,
                json={
                    "inputs": text,
                    "parameters": {
                        "candidate_labels": [toxic_label, safe_label],
                        "multi_label": False
                    }
                },
            )

            if response.status_code == 200:
                result = response.json()

                if isinstance(result, list) and result and isinstance(result[0], dict):
                    top_result = result[0]
                    top_label = top_result.get("label")
                    top_score = top_result.get("score")

                    if top_label and top_score is not None:
                        if top_label == toxic_label and top_score > 0.60:
                            return "toxic"
                        elif top_label == toxic_label and top_score <= 0.60:
                            return "uncertain"
                        else:
                            return "non-toxic"
                
                # ** Ø­Ø§Ù„Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¥Ø°Ø§ Ø¹Ø§Ø¯Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ (Zero-Shot Classification) **
                elif isinstance(result, dict):
                    labels = result.get("labels", [])
                    scores = result.get("scores", [])
                    
                    if labels and scores:
                        top_label = labels[0]
                        top_score = scores[0]
                        if top_label == toxic_label and top_score > 0.60:
                            return "toxic"
                        elif top_label == toxic_label and top_score <= 0.60:
                            return "uncertain"
                        else:
                            return "non-toxic"
                
                else:
                    logging.error(f"Toxicity API returned unexpected type: {type(result)}. Full result: {result}")


            elif response.status_code == 503:
                logging.info("Model loading, skipping toxicity check")
            else:
                logging.error(f"Toxicity API error: {response.status_code} - {response.text}")

        except Exception as e:
            logging.error(f"Toxicity analysis error: {e}")

        return "non-toxic"

    @staticmethod
    def classify_review(sentiment: str, toxicity: str, stars: int = None, text: str = "") -> str:
        sentiment_normalized = sentiment.lower() if sentiment else "neutral"
        is_negative_sentiment = sentiment_normalized in ["Ø³Ù„Ø¨ÙŠ", "negative", "label_0"]
        is_positive_sentiment = sentiment_normalized in ["Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "positive", "label_1"]
        is_toxic = toxicity == "toxic"

        if stars is not None:
            if stars <= 2:
                is_negative_sentiment = True
                is_positive_sentiment = False
            elif stars >= 4:
                is_negative_sentiment = False
                is_positive_sentiment = True

        complaint_keywords = [
            "Ø³Ø±Ù‚", "Ø§Ù†ØªØ²Ø¹", "Ø®Ø¯Ø¹", "ØºØ´", "Ø²Ø¨Ø§Ù„Ø©", "Ø®Ø±ÙŠ", "Ø¨Ø®Ø±Ø§", "Ù…Ø§ Ø¨Ù†ØµØ­",
            "Ø¬Ø±Ø¨Ø§Ù†", "ÙƒØ°Ø¨", "ØºÙŠØ± ØµØ§Ø¯Ù‚", "Ø³ÙŠØ¡", "Ø±Ø¯ÙŠØ¡", "Ù…Ø´ Ø­Ù„Ùˆ", "Ù…Ø´ Ø·ÙŠØ¨"
        ]
        has_complaint_words = any(keyword in text.lower() for keyword in complaint_keywords)

        if is_negative_sentiment or has_complaint_words:
            if is_toxic or has_complaint_words:
                return "Ø´ÙƒÙˆÙ‰"
            else:
                return "Ù†Ù‚Ø¯"
        elif is_positive_sentiment:
            return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
        else:
            if stars is not None:
                if stars <= 2:
                    return "Ù†Ù‚Ø¯"
                elif stars >= 4:
                    return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
            return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"

    @staticmethod
    def detect_review_quality( enjoy_most: str, improve_product: str, additional_feedback: str) -> dict:
        flags = []
        quality_score = 1.0
        all_text = f"{enjoy_most} {improve_product} {additional_feedback}".strip()

        if not all_text or len(all_text.strip()) < 3:
            return {
                'quality_score': 0.0,
                'flags': ['empty_content'],
                'is_suspicious': True
            }

        try:
            arabic_chars = sum(1 for c in all_text if '\u0600' <= c <= '\u06FF')
            english_chars = sum(1 for c in all_text if c.isascii() and c.isalpha())
            total_alpha = arabic_chars + english_chars
            if total_alpha < len(all_text) * 0.3:
                flags.append('gibberish_content')
                quality_score -= 0.3
        except Exception as e:
            logging.error(f"Language detection error: {e}")

        if total_alpha > 500:
            flags.append('too_long')
            quality_score -= 0.1

        if re.search(r'(.)\1{4,}', all_text):
            flags.append('repetitive_characters')
            quality_score -= 0.2

        special_chars = sum(1 for c in all_text if not c.isalnum() and not c.isspace() and c not in '.,!?Ø›ØŒ')
        if special_chars > len(all_text) * 0.2:
            flags.append('excessive_special_chars')
            quality_score -= 0.2

        toxicity_score = SentimentService.analyze_toxicity(all_text)
        if toxicity_score == "toxic":
            flags.append('high_toxicity')
            quality_score -= 0.4

        words = all_text.split()
        if len(words) < 2:
            flags.append('too_short')
            quality_score -= 0.1

        if len(set(words)) < len(words) * 0.5:
            flags.append('repetitive_words')
            quality_score -= 0.2

        quality_score = max(0, quality_score)
        return {
            'quality_score': round(quality_score, 2),
            'flags': flags,
            'is_suspicious': quality_score < 0.5 or len(flags) > 2
        }

    @staticmethod
    def detect_context_mismatch(text: str, shop_type: str) -> dict:
        """ÙŠÙƒØªØ´Ù Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ØºÙŠØ± Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø¯."""
        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        url = HF_TOXICITY_MODEL_URL

        shop_types_arabic = {
            "Ù…Ø·Ø¹Ù…": "Ù…Ø·Ø¹Ù… ÙˆØ£ÙƒÙ„ ÙˆÙ…Ø´Ø±ÙˆØ¨Ø§Øª",
            "Ù…Ù‚Ù‡Ù‰": "Ù…Ù‚Ù‡Ù‰ ÙˆÙ‚Ù‡ÙˆØ© ÙˆÙ…Ø´Ø±ÙˆØ¨Ø§Øª",
            "Ù…Ø­Ù„ Ù…Ù„Ø§Ø¨Ø³": "Ù…Ù„Ø§Ø¨Ø³ ÙˆØ£Ø²ÙŠØ§Ø¡ ÙˆÙ…ÙˆØ¶Ø©",
            "ØµÙŠØ¯Ù„ÙŠØ©": "ØµÙŠØ¯Ù„ÙŠØ© ÙˆØ£Ø¯ÙˆÙŠØ© ÙˆØ¹Ù„Ø§Ø¬",
            "Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙƒØª": "Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙƒØª ÙˆØªØ³ÙˆÙ‚ ÙˆÙ…Ù†ØªØ¬Ø§Øª",
            "Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª": "Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ£Ø¬Ù‡Ø²Ø© ÙˆØªÙ‚Ù†ÙŠØ©",
            "Ù…ÙƒØªØ¨Ø©": "ÙƒØªØ¨ ÙˆÙ‚Ø±Ø§Ø¡Ø© ÙˆØªØ¹Ù„ÙŠÙ…",
            "Ù…Ø­Ù„ ØªØ¬Ù…ÙŠÙ„": "ØªØ¬Ù…ÙŠÙ„ ÙˆØ´Ø¹Ø± ÙˆØ¨Ø´Ø±Ø©",
            "ØµØ§Ù„Ø© Ø±ÙŠØ§Ø¶ÙŠØ©": "Ø±ÙŠØ§Ø¶Ø© ÙˆØªÙ…Ø§Ø±ÙŠÙ† ÙˆÙ„ÙŠØ§Ù‚Ø©",
            "Ù…Ø¯Ø±Ø³Ø©": "Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ¹Ù„ÙŠÙ… ÙˆØ·Ù„Ø§Ø¨",
            "Ù…Ø³ØªØ´ÙÙ‰": "Ø·Ø¨ ÙˆØ¹Ù„Ø§Ø¬ ÙˆÙ…Ø±Ø¶Ù‰",
            "Ù…Ø­Ø·Ø© ÙˆÙ‚ÙˆØ¯": "ÙˆÙ‚ÙˆØ¯ ÙˆØ³ÙŠØ§Ø±Ø§Øª ÙˆØ¨Ù†Ø²ÙŠÙ†",
            "Ù…ØªØ¬Ø± Ø£Ø¬Ù‡Ø²Ø©": "Ø£Ø¬Ù‡Ø²Ø© ÙˆØ¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØªÙ‚Ù†ÙŠØ©",
            "Ù…Ø­Ù„ Ø£Ù„Ø¹Ø§Ø¨": "Ø£Ù„Ø¹Ø§Ø¨ ÙˆØªØ±ÙÙŠÙ‡ ÙˆØ£Ø·ÙØ§Ù„",
            "Ù…ÙƒØªØ¨ Ø³ÙŠØ§Ø­ÙŠ": "Ø³ÙØ± ÙˆØ³ÙŠØ§Ø­Ø© ÙˆÙÙ†Ø§Ø¯Ù‚",
            "Ù…Ø­Ù„ Ù‡Ø¯Ø§ÙŠØ§": "Ù‡Ø¯Ø§ÙŠØ§ ÙˆØªØ°ÙƒØ§Ø±Ø§Øª ÙˆÙ…Ù†Ø§Ø³Ø¨Ø§Øª",
            "Ù…ØºØ³Ù„Ø© Ù…Ù„Ø§Ø¨Ø³": "ØºØ³ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ ÙˆÙ…Ù„Ø§Ø¨Ø³",
            "Ù…ØªØ¬Ø± Ù‡ÙˆØ§ØªÙ": "Ù‡ÙˆØ§ØªÙ ÙˆÙ…ÙˆØ¨Ø§ÙŠÙ„Ø§Øª ÙˆØªÙ‚Ù†ÙŠØ©",
            "Ø¹Ø§Ù…": "Ù†Ø´Ø§Ø· ØªØ¬Ø§Ø±ÙŠ Ø¹Ø§Ù…"
        }

        target_label = shop_types_arabic.get(shop_type, shop_type)
        general_label = "Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙˆØªØ¹Ø§Ù…Ù„ Ø¹Ø§Ù… ÙˆÙ†Ø¸Ø§ÙØ©"
        other_label = "Ø³ÙŠØ§Ù‚ Ø¢Ø®Ø± ØºÙŠØ± Ù…Ø±ØªØ¨Ø·"

        candidate_labels = [target_label, general_label, other_label]

        payload = {
            "inputs": text,
            "parameters": {
                "candidate_labels": candidate_labels,
                "multi_label": False
            }
        }

        try:
            response = requests.post(url, headers=headers, json=payload)

            if response.status_code == 503:
                logging.info("Model is loading, waiting...")
                import time
                time.sleep(20)
                # Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
                response = requests.post(url, headers=headers, json=payload)

            if response.status_code == 200:
                result = response.json()
                labels = []
                scores = []
                
                # ğŸ› ï¸ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙƒÙ‚Ø§Ø¦Ù…Ø© (List)
                if isinstance(result, list):
                    for item in result:
                        if isinstance(item, dict) and 'label' in item and 'score' in item:
                            labels.append(item['label'])
                            scores.append(item['score'])
                
                # Ø­Ø§Ù„Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù‚Ø§Ù…ÙˆØ³ (Zero-Shot Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ)
                elif isinstance(result, dict):
                    labels = result.get("labels", [])
                    scores = result.get("scores", [])
                
                else:
                    logging.error(f"Context API returned unexpected type: {type(result)}. Full result: {result}")

                if labels and scores:
                    # ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£ØµÙ„ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ labels Ùˆ scores
                    result_map = {label: score for label, score in zip(labels, scores)}

                    target_score = result_map.get(target_label, 0)
                    general_score = result_map.get(general_label, 0)
                    other_score = result_map.get(other_label, 0)

                    valid_relevance = target_score + general_score
                    has_mismatch = valid_relevance < 0.40

                    return {
                        'mismatch_score': round(other_score, 2),
                        'confidence': round(valid_relevance * 100, 2),
                        'reasons': [f"Ø§Ù„Ù†Øµ ÙŠØ¨Ø¯Ùˆ Ø¨Ø¹ÙŠØ¯Ø§Ù‹ Ø¹Ù† Ø³ÙŠØ§Ù‚ {shop_type} Ø£Ùˆ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø©"] if has_mismatch else [],
                        'has_mismatch': has_mismatch,
                        'predicted_label': labels[0] if labels else None
                    }
            else:
                logging.error(f"HF API Error: {response.status_code} - {response.text}")

        except Exception as e:
            logging.error(f"Context mismatch detection error: {e}")

        return {
            'mismatch_score': 0.0,
            'confidence': 100.0,
            'reasons': 'Ù„Ø§Ø´ÙŠØ¡',
            'has_mismatch': False,
            'predicted_label': "Error"
        }    
    @staticmethod
    def analyze_review_comprehensive(self,dto: ReviewDTO, shop_type: str) -> SentimentAnalysisResultDTO:
        cleaned_enjoy_most = SentimentService.clean_text(dto.enjoy_most or "")
        cleaned_improve_product = SentimentService.clean_text(dto.improve_product or "")
        cleaned_feedback = SentimentService.clean_text(dto.additional_feedback or "")
        
        full_text_parts = [
            f"Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ø¬ÙˆÙ…: {dto.stars}" if dto.stars else "",
            f"Ø£ÙƒØ«Ø± Ù…Ø§ Ø£Ø¹Ø¬Ø¨Ù†ÙŠ: {cleaned_enjoy_most}" if cleaned_enjoy_most else "",
            f"Ø§Ù‚ØªØ±Ø§Ø­ Ù„Ù„ØªØ­Ø³ÙŠÙ†: {cleaned_improve_product}" if cleaned_improve_product else "",
            f"Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: {cleaned_feedback}" if cleaned_feedback else ""
        ]
        full_text = " | ".join([part for part in full_text_parts if part]).strip()

        if not full_text:
            full_text = cleaned_text

        sentiment = SentimentService.analyze_sentiment(full_text)
        toxicity = SentimentService.analyze_toxicity(full_text)
        category = SentimentService.classify_review(
            sentiment=sentiment,
            toxicity=toxicity,
            stars=dto.stars,
            text=full_text
        )

        quality_result = SentimentService.detect_review_quality(
            enjoy_most=dto.enjoy_most or "",
            improve_product=dto.improve_product or "",
            additional_feedback=dto.additional_feedback or ""
        )

        context_result = SentimentService.detect_context_mismatch(full_text, shop_type)

        is_spam = quality_result.get('is_suspicious', False)
        context_match = not context_result.get('has_mismatch', False)

        return SentimentAnalysisResultDTO(
            sentiment=sentiment,
            toxicity=toxicity,
            category=category,
            quality_score=quality_result.get('quality_score', 1.0),
            is_spam=is_spam,
            context_match=context_match,
            quality_flags=quality_result.get('flags', []),
            mismatch_reasons=context_result.get('reasons', [])
        )

    @staticmethod
    def detect_and_censor_profanity_in_review(
        text: str = "",
        enjoy_most: str = "",
        improve_product: str = "",
        additional_feedback: str = "",
        use_hf: bool = True
    ) -> dict:
        result = {
            'text': {
                'original': text,
                'censored': text,
                'has_profanity': False,
                'profanity_score': 0.0,
                'censored_words': []
            },
            'enjoy_most': {
                'original': enjoy_most,
                'censored': enjoy_most,
                'has_profanity': False,
                'censored_words': []
            },
            'improve_product': {
                'original': improve_product,
                'censored': improve_product,
                'has_profanity': False,
                'censored_words': []
            },
            'additional_feedback': {
                'original': additional_feedback,
                'censored': additional_feedback,
                'has_profanity': False,
                'censored_words': []
            },
            'summary': {
                'total_fields_with_profanity': 0,
                'total_censored_words': [],
                'has_any_profanity': False,
                'overall_profanity_score': 0.0,
                'method': 'hf' if use_hf else 'regex'
            }
        }

        fields = [
            ('text', text),
            ('enjoy_most', enjoy_most),
            ('improve_product', improve_product),
            ('additional_feedback', additional_feedback)
        ]

        total_censored = []
        fields_with_profanity = 0
        total_score = 0.0

        for field_name, field_text in fields:
            if field_text and field_text.strip():
                if use_hf and field_name == 'text':
                    profanity_details = TextProfanityService.detect_profanity_with_hf(field_text)
                    has_profanity = profanity_details['has_profanity']
                    profanity_score = profanity_details['profanity_score']
                else:
                    profanity_details = TextProfanityService._detect_profanity_with_patterns(field_text)
                    has_profanity = profanity_details['has_profanity']
                    profanity_score = profanity_details['profanity_score']

                censored_text, censored_words = TextProfanityService.censor_profanity(
                    field_text,
                    censor_char='*',
                    method='word'
                )

                result[field_name]['censored'] = censored_text
                result[field_name]['has_profanity'] = has_profanity
                result[field_name]['profanity_score'] = profanity_score
                result[field_name]['censored_words'] = censored_words

                if has_profanity:
                    fields_with_profanity += 1
                    total_score += profanity_score

                total_censored.extend(censored_words)

        result['summary']['total_fields_with_profanity'] = fields_with_profanity
        result['summary']['total_censored_words'] = list(set(total_censored))
        result['summary']['has_any_profanity'] = fields_with_profanity > 0
        if fields_with_profanity > 0:
            result['summary']['overall_profanity_score'] = round(total_score / fields_with_profanity, 3)

        return result
